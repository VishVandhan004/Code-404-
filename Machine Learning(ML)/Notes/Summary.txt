We should be using the jupyter notebooks for the implementation of all the Data-science and machine learning projects.

Numpy -> Numpy is a python library which is used for quick mathematical operations like mean, standard deviation, median, etc.
It is used for working with arrays. It stands for numerical python. It is basically 50x faster than the normal python lists.
Exp -> one basic example of numpy is ->
import numpy as np
arr = np.array([1,2,3,4,5])
print(arr)
0-D array-> This is basically a scalar quantity with only one value..
exp -> arr = np.array(42)
1-D array -> It contains more than one value in it.
exp -> arr = np.array([1,2,3,4,5])
2-D array -> It has 2 arrays in it. has 2-dimensions.
exp-> arr=np.array([1,2,3],[4,5,6])
3-D array -> it is the collection of 2 different 2-d arrays
We can find out the dimensions of the arrays also.
import numpy as np
a = np.array(42)        ->0-D
b = np.array([1,2,3,4,5]) -> 1-D
c = np.array([[1,2,3],[4,5,6]]) ->2-D
d = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]]) -> 3-D
print(a.ndim)
print(b.ndim)
print(c.ndim)
print(d.ndim)
Shape -> It is the number of elements in each dimension
Reshaping -> it is to change the shape of the array.
Flattening -> Converting a multi-dimensional array into a 1-D array.
Searching in arrays -> we use the where() method to find out stuff in the array
Sort in arrays -> In numpy, we use sort(), to put elements in an order..
exp-> Some Array Stuff
import numpy as np
a=np.array([1,2,3,4])
print(a.shape)   #Shape of the Array 'a'
b = np.array([[1,2,3],[4,5,6]])
print(b.shape)  # Shape of the Array 'b'
c = np.array([1,2,3,4,5,6,7,8,9,10,11,12])
print(c.shape)  #Shape of the Array 'c'
new_c = c.reshape(4,3) #reshaping into 4 arrays which has 3 elements in it
print(new_c)
new2_c = c.reshape(2,3,2) # For a 3-D array
print(new2_c)
f_arr=b.reshape(-1)  # For flattening a multi-dimensional array
print(f_arr)
a = np.array([3,4,5,6,7,8,9])
a[0]    # Accessing the elements in the list..
a[0] + a[2]
for i in a:
    print(i) #Iterating through the 1-D Array
print(a[1:4]) #last index(3) won't be considered for slicing
print(a[2:])
print(a[:5])
print(a[1:5:2]) #prints from 1st index to 5th index but takes 2 gaps.
b = np.array([[1,2,3,4,5],[6,7,8,9,10]])
print(b)
print(b[1,4]) # accessing 5th element from 2nd row..
print(b[0,3]) # accessing 4th element from 1st row..
for j in b:
    print(j) #Iterating through the 2-D Array
for j in b:
    for k in j:  #Iterating and getting the elements in the 2-D Array
        print(k)
print(b[1,1:4]) #prints the elements from 1st to 4th index in the 2nd column.
print(b[0,0:3])
d = np.array([2,4,1,6,4,7,9,1])
x = np.where(d == 1)
print(x) # shows where is 1 present in the whole array
ans = np.where(d%2==0) # which index has even numbers and will return the indexes
print(ans)
print(np.sort(d)) # sorts the elements in the array.
g = np.array(['fille','femme','garcon'])
print(np.sort(g))


Pandas -> Pandas are used for data cleaning and preprocessing tasks before data modeling. Very important for Data-Science. we can clean, explore, manipulate the data using pandas. Pandas is the primary tool data scientists use for exploring and manipulating data
PANDAS -> "Panel data" + "Python Data Analysis"
It allows us to analyze big data and make conclusions.
exp-> deleting rows and columns which are empty, NULL, or not relevant.
To do all the stuff like clean, manipulate, etc you need a .CSV file for pandas which can be read by using df or 'data frame'. It will store the data present in a .CSV file into the df.
Pandas Series -> It is like a column in a table. it's a 1-D array holding any type of data.
Pandas Dataframe -> It's a 2-d data structure which has rows and columns.
It basically converts the data of excel or .CSV and stores it in the data according to the pandas data frame.
Locate -> use the 'loc' attribute to return one or more specified rows.
exp ->
import pandas as pd
l1 = [3,6,8,10,30,22]
s1 = pd.Series(l1)
print(s1)
d1 = {"one":10,"two":20,"three":30}
s2 = pd.Series(d1)
print(s2)
sample_data = {
    "weekdays":['Monday','Tuesday','Friday'],
    "test_score":[10,30,50]
}
df = pd.DataFrame(sample_data)
print(df)
print(df.loc[0])
print(df.loc[1])
CSV (Comma seperated files) -> It's an easy way to store big data-sets and can easily be read by pandas.
If the .csv file is in the same location as of the notebook, you can simply put the name of the file. 
If it's not in the same location, then we need to copy the whole path and then access it.
head() -> it returns the rows(top-5) from the start.
tail() -> it returns the last rows.
info() -> it returns the basic info of the data-set.
describe() -> it returns the description of the data in df. it gives mean, std, min, 25th,50th,75th percentiles, max, etc..
corr() -> it describes the relationship between each column in the data-set. (Co-Relation)
exp->
import pandas as pd
df = pd.read_csv('covid_worldwide.csv')
print(df)
print(df.head()) #returns top-5 values from the start
print(df.tail()) #returns last-5 values from the end
print(df.info()) #gives basic info about data-set.
print(df.describe()) #gives some values.


Matplotlib and Seaborn are used for data visualization.
Matplotlib is a python library which is used for making various data analysis.
The matplotlib utilities lie under a sub-plot called 'pyplot'.
We can make charts and graphs like line, bar etc..
plot() ->used to plot the graphs using both x&y axes.
show() -> used to show the graph after plotting.
exp -> 
import matplotlib.pyplot as plt
import numpy as np
x = np.array([4,7,2,10,13])
plt.plot(x)    # line-plot
plt.show()
y = np.array([1,2,3,4,5])
plt.plot(y,linestyle='dashed') # line-plot with dashes
plt.show()
a = np.array(['A','B','C','D'])
b = np.array([10,40,50,20])
plt.bar(a,b)          # Bar-chart
plt.show()
x1 = np.array([20,15,11,7,8,10])
y1 = np.array([2,7,15,8,10,12])
plt.scatter(x1,y1)     # Scatter-Plot
plt.show()
p = np.array([30,20,25,20])
plt.pie(p)        # Pie-Chart
plt.show()
h1 = np.random.normal(170,10,250)
plt.hist(h1)     # Histogram
plt.show()


SeaBorn
Boxplot -> it is used to show the data in the form of quartiles ( 25th, 50th, 75th) and also gives the outliers.. It consists max, min, etc..
Reg-plot -> used for linear regression models. it gives the best fit-line and scatter-plot.
exp ->
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
df = sns.load_dataset("tips")
df
df.boxplot(by='day', column = ['total_bill'], grid = False) # box-plot
titanic = sns.load_dataset("titanic")
titanic.head()
age1 = titanic['age'].dropna()  # removes all the null values from the age column and stores the new values in age1
sns.distplot(age1,bins=30,kde=False)
plt.show()
data = sns.load_dataset("mpg")
data.head()
sns.regplot(x='mpg',y='acceleration',data = data) # reg-plot
plt.show()


Missing values -> Some values are missing in the table. 
We can replace the missing values with the mean/mode.
Replacing it with mode -> 
exp -> 
import pandas as pd
df = pd.read_csv('covid_worldwide.csv')
print(df)
print(df.isnull().sum()) # this line shows how many columns have missing data.
print(df['Population'] = df['Population'].fillna(df['Population'].mode()[0])) # we are basically taking the mode of the data present in the population column and assigning it to itself. 
print(df.isnull().sum())
Replacing it with mean -> 
import numpy as np
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values = np.nan, strategy='mean')
data = [[12,np.nan,34],[10,32,np.nan]]
print("Original Data: \n",data)
imputer = imputer.fit(data)
data = imputer.transform(data)
print("Imputer Data: \n",data)

Encoding -> Storing the values in the data-sets in such a way that the ML-Model can understand.
1. Nominal -> pen, pencil, eraser,etc (order doesn't matter)
2. Ordinal -> in ratings -> good, bad, shit (order matters here)
One-Hot Encoding -> used to get dummy data in a column
import pandas as pd
df = pd.read_csv('covid_worldwide.csv')
df.head()
df['Country'].value_counts()
pd.get_dummies(df,columns=['Country'])
LabelEncoder ->
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder() # this is used for pre-processing the data in a column.If there is a text in a whole column, the Ml-Model wouldn't understand it. Label encoder is used to convert the text into the numbers.
Feature Scaling -> It is a technique used to standardize the independent features in the data in a fixed range. It's performed during data pre-process


Machine Learning -> It's a subset of Artificial Intelligence which is mainly concerned with the development of algorithms which allow a computer to learn from the data(training data) and past shit and make future predictions without being explicitly programmed. The ML algorithms build a mathematical model using these data.
types ->
Supervised, unsupervised, Reinforcement.

Supervised -> where the machines are trained using the 'labelled data' and predicts the output. Training/labelled data acts as the superviser.
80% data for training, 20% data for testing..
exp -> Spam filter, etc.
types -> Regression and Classifcation

Unsupervised -> Models are trained using unlabeled data and are allowed to act on the data without any supervision. It can be compared to human brain , where it learns new things every single time.
Raw data is given for input, no training, testing will happen like supervised.
types -> Clustering and Association.

Train-Test (Supervised ML) -> It's a procedure that allows you to simulate how a model would perform on new/unseen data.

Regression -> It is the statistical method to model the relationship between a dependent and an independent variable. How the value of dependent variable changes corresponding to the independent variable.
Overfitting -> Performing well in training but not in testing.
Underfitting -> Not performing well in both testing and training.

Linear Regression ->
It shows the linear relationship between the independent(x) and dependent(y) variable.
It makes predictions for continous/real numeric values. It provides a straight line. y=ax+b( general form of L-R ML)
Positive LR -> dependent variable increases on Y-axis, independent variable increases on X-axis
Negative LR -> Dependent decreases on Y-axis, independent increases on X-axis
exp->
#LINEAR REGRESSION
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
data_set = pd.read_csv('1000_Companies.csv')
data_set
x = data_set.iloc[:,:-1].values
y = data_set.iloc[:,1].values
x
y
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(x_train,y_train)
y_pred = reg.predict(x_test)
x_pred = reg.predict(x_train)
plt.scatter(x_train,y_train,color="green")
plt.plot(x_train,x_pred,color="red")
plt.show()            # TRAINING DATASET
plt.scatter(x_test,y_test,color='blue')
plt.plot(x_train,x_pred,color="red")
plt.show()               # TESTING DATASET

Logisitical Regression ->
It predicts the output of a categorical dependent variable. it must be a categorical or discrete value. It can be either yes or no , 0 or 1, True or False. It can also give probabilistic values between 0 and 1. It basically forma a S-curve. The S-form curve is called the sigmoid function.
exp->
import pandas as pd
import numpy as np
df = pd.read_csv('1000_Companies.csv')
df.head()
x = df.iloc[:,2].values
y = df.iloc[:,4].values
from sklearn.preprocessing import StandardScaler
st_x = StandardScaler()
x_train = st_x.fit_transform(x_train)
x_test = st.x_transform(x_test)
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression()
clf.fit(x_train,y_train)
clf.fit(x_train,y_train)

K Nearest Neighbours(K-NN) -> It stores all the available data and classifies a new data point based on the similarity. Mostly used in Classification.
It's a lazy-leaner algorithm. It doesn't learn quickly from the given data.
follow the algo for calculation ->
1. select the no.of.neighbours of k
2. calculate the ecludian distance of k no.of.neighbours.
3.count the no.of.data points in each category,
assign where the no.of.neighbours are max.

SVM (Support Vector machine) -> it is used for classification and regression problems. the goal is to create the best line so that the new data point will be present in the best or correct category in the future. All these can be represented by vectors, so its called SVM. it has 2 types. they are ->
Linear SVM -> it used for linearly separable data, dataset is classified into 2 classes by using a straight line.
Non-Linear SVM -> it is used for non-linearly separable data, no straight line.
Hyperplane -> there can be multiple lines to segregate the classes in the n-dimensional plane, but the best line for classification of data points must be found. This best line is hyperplane.
Support Vectors -> the data points that are closest to the hyperplane and affect the position of the hyperplane is called support vectors.

Decision Tree -> It is used for both classification and regression. it's a tree structured classifier, internal nodes represent features of a dataset,branches represent decision rules, leaf nodes is for outcome.
it has 2 nodes, decision(has multiple branches) and leaf nodes(no further branches). we use CART algorithm -> classification and regression tree algorithm.
ex-> whether the person will accept the job offer or not or whether he'll buy it or not.

Random-Forest -> It is a supervised ML. it's a process of combining multiple classifiers to solve a complex problem and to improve performance of model. it combines no.of.decision trees on various subsets of given dataset and takes the average to predictive accuracy of the dataset. 
exp-> identifying an apple in a bunch of fruits accurately.

K-Means Clustering -> It is the first Unsupervised ML algo. It groups un-labeled data into clusters. It divides the unlabeled data into k different clusters in such a way that one group has similar properties. we don't have training, testing here.
working ->
1.select number K to decide the clusters.
2. select random k points or centroids.
3.assign each data point to closest centroid.
4. calculate the variance and place a new centroid.
5. repeat third steps, reassigning each datapoint to the new closest centroid of cluster.
6. Model is ready

Grid-Search CV -> 
Parametres are the variables that an algorithm itself produces to produce a prediction.
Hyper-Parametres are the variables that you specify while building a ML model.
Hyper-parameter tuning refers to the process of finding hyper-parametres that yield the best result. One type is using Grid-search CV.

Machine-Learning Workflow
1.Ingestion -> load the dataset in the form of .CSV files , etc.
2. Cleaning.
3. preprocessing.
4. Modeling.
5. Deployment. (AWS or cloud)
ML Pipeline is a process of automating the workflow of a complete ML task.


                                  **************** KAGGLE EXAMPLE *************************
                    # Code you have previously used to load data
import pandas as pd
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor


# Path of the file to read
iowa_file_path = '../input/home-data-for-ml-course/train.csv'

home_data = pd.read_csv(iowa_file_path)
# Create target object and call it y
y = home_data.SalePrice
# Create X
features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']
X = home_data[features]

# Split into validation and training data
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)

# Specify Model
iowa_model = DecisionTreeRegressor(random_state=1)
# Fit Model
iowa_model.fit(train_X, train_y)

# Make validation predictions and calculate mean absolute error
val_predictions = iowa_model.predict(val_X)
val_mae = mean_absolute_error(val_predictions, val_y)
print("Validation MAE when not specifying max_leaf_nodes: {:,.0f}".format(val_mae))

# Using best value for max_leaf_nodes
iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)
iowa_model.fit(train_X, train_y)
val_predictions = iowa_model.predict(val_X)
val_mae = mean_absolute_error(val_predictions, val_y)
print("Validation MAE for best value of max_leaf_nodes: {:,.0f}".format(val_mae))

# We can also do it using the random forest.
from sklearn.ensemble import RandomForestRegressor

# Define the model. Set random_state to 1
rf_model = RandomForestRegressor(random_state = 1)

# fit your model
rf_model.fit(train_X, train_y)
rf_val_predictions = rf_model.predict(val_X)

# Calculate the mean absolute error of your Random Forest model on the validation data
rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)

print("Validation MAE for Random Forest Model: {}".format(rf_val_mae))

# Check your answer
step_1.check()


# Set up code checking
from learntools.core import binder
binder.bind(globals())
from learntools.machine_learning.ex6 import *
print("\nSetup complete")