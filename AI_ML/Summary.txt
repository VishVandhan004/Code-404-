We should be using the jupyter notebooks for the implementation of all the Data-science and machine learning projects.

Numpy -> Numpy is a python library which is used for quick mathematical operations like mean, standard deviation, median, etc.
It is used for working with arrays. It stands for numerical python. It is basically 50x faster than the normal python lists.
Exp -> one basic example of numpy is ->
import numpy as np
arr = np.array([1,2,3,4,5])
print(arr)
0-D array-> This is basically a scalar quantity with only one value..
exp -> arr = np.array(42)
1-D array -> It contains more than one value in it.
exp -> arr = np.array([1,2,3,4,5])
2-D array -> It has 2 arrays in it. has 2-dimensions.
exp-> arr=np.array([1,2,3],[4,5,6])
3-D array -> it is the collection of 2 different 2-d arrays
We can find out the dimensions of the arrays also.
import numpy as np
a = np.array(42)        ->0-D
b = np.array([1,2,3,4,5]) -> 1-D
c = np.array([[1,2,3],[4,5,6]]) ->2-D
d = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]]) -> 3-D
print(a.ndim)
print(b.ndim)
print(c.ndim)
print(d.ndim)
Shape -> It is the number of elements in each dimension
Reshaping -> it is to change the shape of the array.
Flattening -> Converting a multi-dimensional array into a 1-D array.
Searching in arrays -> we use the where() method to find out stuff in the array
Sort in arrays -> In numpy, we use sort(), to put elements in an order..
exp-> Some Array Stuff
import numpy as np
a=np.array([1,2,3,4])
print(a.shape)   #Shape of the Array 'a'
b = np.array([[1,2,3],[4,5,6]])
print(b.shape)  # Shape of the Array 'b'
c = np.array([1,2,3,4,5,6,7,8,9,10,11,12])
print(c.shape)  #Shape of the Array 'c'
new_c = c.reshape(4,3) #reshaping into 4 arrays which has 3 elements in it
print(new_c)
new2_c = c.reshape(2,3,2) # For a 3-D array
print(new2_c)
f_arr=b.reshape(-1)  # For flattening a multi-dimensional array
print(f_arr)
a = np.array([3,4,5,6,7,8,9])
a[0]    # Accessing the elements in the list..
a[0] + a[2]
for i in a:
    print(i) #Iterating through the 1-D Array
print(a[1:4]) #last index(3) won't be considered for slicing
print(a[2:])
print(a[:5])
print(a[1:5:2]) #prints from 1st index to 5th index but takes 2 gaps.
b = np.array([[1,2,3,4,5],[6,7,8,9,10]])
print(b)
print(b[1,4]) # accessing 5th element from 2nd row..
print(b[0,3]) # accessing 4th element from 1st row..
for j in b:
    print(j) #Iterating through the 2-D Array
for j in b:
    for k in j:  #Iterating and getting the elements in the 2-D Array
        print(k)
print(b[1,1:4]) #prints the elements from 1st to 4th index in the 2nd column.
print(b[0,0:3])
d = np.array([2,4,1,6,4,7,9,1])
x = np.where(d == 1)
print(x) # shows where is 1 present in the whole array
ans = np.where(d%2==0) # which index has even numbers and will return the indexes
print(ans)
print(np.sort(d)) # sorts the elements in the array.
g = np.array(['fille','femme','garcon'])
print(np.sort(g))


Pandas -> Pandas are used for data cleaning and preprocessing tasks before data modeling. Very important for Data-Science. we can clean, explore, manipulate the data using pandas. 
PANDAS -> "Panel data" + "Python Data Analysis"
It allows us to analyze big data and make conclusions.
exp-> deleting rows and columns which are empty, NULL, or not relevant.
To do all the stuff like clean, manipulate, etc you need a .CSV file for pandas which can be read by using df or 'data frame'. It will store the data present in a .CSV file into the df.
Pandas Series -> It is like a column in a table. it's a 1-D array holding any type of data.
Pandas Dataframe -> It's a 2-d data structure which has rows and columns.
It basically converts the data of excel or .CSV and stores it in the data according to the pandas data frame.
Locate -> use the 'loc' attribute to return one or more specified rows.
exp ->
import pandas as pd
l1 = [3,6,8,10,30,22]
s1 = pd.Series(l1)
print(s1)
d1 = {"one":10,"two":20,"three":30}
s2 = pd.Series(d1)
print(s2)
sample_data = {
    "weekdays":['Monday','Tuesday','Friday'],
    "test_score":[10,30,50]
}
df = pd.DataFrame(sample_data)
print(df)
print(df.loc[0])
print(df.loc[1])
CSV (Comma seperated files) -> It's an easy way to store big data-sets and can easily be read by pandas.
If the .csv file is in the same location as of the notebook, you can simply put the name of the file. 
If it's not in the same location, then we need to copy the whole path and then access it.
head() -> it returns the rows(top-5) from the start.
tail() -> it returns the last rows.
info() -> it returns the basic info of the data-set.
describe() -> it returns the description of the data in df. it gives mean, std, min, 25th,50th,75th percentiles, max, etc..
corr() -> it describes the relationship between each column in the data-set. (Co-Relation)
exp->
import pandas as pd
df = pd.read_csv('covid_worldwide.csv')
print(df)
print(df.head()) #returns top-5 values from the start
print(df.tail()) #returns last-5 values from the end
print(df.info()) #gives basic info about data-set.
print(df.describe()) #gives some values.


Matplotlib and Seaborn are used for data visualization.
Matplotlib is a python library which is used for making various data analysis.
The matplotlib utilities lie under a sub-plot called 'pyplot'.
We can make charts and graphs like line, bar etc..
plot() ->used to plot the graphs using both x&y axes.
show() -> used to show the graph after plotting.
exp -> 
import matplotlib.pyplot as plt
import numpy as np
x = np.array([4,7,2,10,13])
plt.plot(x)    # line-plot
plt.show()
y = np.array([1,2,3,4,5])
plt.plot(y,linestyle='dashed') # line-plot with dashes
plt.show()
a = np.array(['A','B','C','D'])
b = np.array([10,40,50,20])
plt.bar(a,b)          # Bar-chart
plt.show()
x1 = np.array([20,15,11,7,8,10])
y1 = np.array([2,7,15,8,10,12])
plt.scatter(x1,y1)     # Scatter-Plot
plt.show()
p = np.array([30,20,25,20])
plt.pie(p)        # Pie-Chart
plt.show()
h1 = np.random.normal(170,10,250)
plt.hist(h1)     # Histogram
plt.show()


SeaBorn
Boxplot -> it is used to show the data in the form of quartiles ( 25th, 50th, 75th) and also gives the outliers.. It consists max, min, etc..
Reg-plot -> used for linear regression models. it gives the best fit-line and scatter-plot.
exp ->
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
df = sns.load_dataset("tips")
df
df.boxplot(by='day', column = ['total_bill'], grid = False) # box-plot
titanic = sns.load_dataset("titanic")
titanic.head()
age1 = titanic['age'].dropna()  # removes all the null values from the age column and stores the new values in age1
sns.distplot(age1,bins=30,kde=False)
plt.show()
data = sns.load_dataset("mpg")
data.head()
sns.regplot(x='mpg',y='acceleration',data = data) # reg-plot
plt.show()


Missing values -> Some values are missing in the table. 
We can replace the missing values with the mean/mode.
Replacing it with mode -> 
exp -> 
import pandas as pd
df = pd.read_csv('covid_worldwide.csv')
print(df)
print(df.isnull().sum()) # this line shows how many columns have missing data.
print(df['Population'] = df['Population'].fillna(df['Population'].mode()[0])) # we are basically taking the mode of the data present in the population column and assigning it to itself. 
print(df.isnull().sum())
Replacing it with mean -> 
import numpy as np
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values = np.nan, strategy='mean')
data = [[12,np.nan,34],[10,32,np.nan]]
print("Original Data: \n",data)
imputer = imputer.fit(data)
data = imputer.transform(data)
print("Imputer Data: \n",data)

Encoding -> Storing the values in the data-sets in such a way that the ML-Model can understand.
1. Nominal -> pen, pencil, eraser,etc (order doesn't matter)
2. Ordinal -> in ratings -> good, bad, shit (order matters here)
One-Hot Encoding -> used to get dummy data in a column
import pandas as pd
df = pd.read_csv('covid_worldwide.csv')
df.head()
df['Country'].value_counts()
pd.get_dummies(df,columns=['Country'])
LabelEncoder ->
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder() # this is used for pre-processing the data in a column.If there is a text in a whole column, the Ml-Model wouldn't understand it. Label encoder is used to convert the text into the numbers.
Feature Scaling -> It is a technique used to standardize the independent features in the data in a fixed range. It's performed during data pre-process


Machine Learning -> It's a subset of Artificial Intelligence which is mainly concerned with the development of algorithms which allow a computer to learn from the data(training data) and past shit and make future predictions without being explicitly programmed. The ML algorithms build a mathematical model using these data.
types ->
Supervised, unsupervised, Reinforcement.

Supervised -> where the machines are trained using the 'labelled data' and predicts the output. Training/labelled data acts as the superviser.
exp -> Spam filter, etc.
types -> Regression and Classifcation
